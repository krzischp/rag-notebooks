{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977cad9-5c46-4836-a58e-6e6ee2c370a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac89ca9-08d4-4f9c-a908-a823f5fcf22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eebf7cae-6e1a-4f40-bed1-0c840fc4e532",
   "metadata": {},
   "source": [
    "[Tutorial link](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf095f85-1b9a-4430-86bc-28d80bc8037e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T00:22:15.686946Z",
     "iopub.status.busy": "2024-04-28T00:22:15.686663Z",
     "iopub.status.idle": "2024-04-28T00:22:15.714787Z",
     "shell.execute_reply": "2024-04-28T00:22:15.714197Z",
     "shell.execute_reply.started": "2024-04-28T00:22:15.686925Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d771622-8d4f-4522-83dd-4ef5eca15c82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T00:22:15.715988Z",
     "iopub.status.busy": "2024-04-28T00:22:15.715698Z",
     "iopub.status.idle": "2024-04-28T00:22:16.245466Z",
     "shell.execute_reply": "2024-04-28T00:22:16.244757Z",
     "shell.execute_reply.started": "2024-04-28T00:22:15.715969Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084e8e01-65bc-4d64-a5fc-b3eefdef02e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T00:22:16.246859Z",
     "iopub.status.busy": "2024-04-28T00:22:16.246359Z",
     "iopub.status.idle": "2024-04-28T00:22:16.263851Z",
     "shell.execute_reply": "2024-04-28T00:22:16.263122Z",
     "shell.execute_reply.started": "2024-04-28T00:22:16.246833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "llamaindex_dir = os.getcwd()\n",
    "# Get the parent directory\n",
    "llamaindex_dir = os.path.dirname(llamaindex_dir)\n",
    "\n",
    "sys.path.append(llamaindex_dir + \"/utils\")\n",
    "# sys.path\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "351708be-dd65-47a1-b281-1e4b2be5265f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T00:22:16.265278Z",
     "iopub.status.busy": "2024-04-28T00:22:16.264835Z",
     "iopub.status.idle": "2024-04-28T00:22:16.280053Z",
     "shell.execute_reply": "2024-04-28T00:22:16.279404Z",
     "shell.execute_reply.started": "2024-04-28T00:22:16.265254Z"
    }
   },
   "outputs": [],
   "source": [
    "from llamaindex_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89691402-64a9-4dab-a536-14e864ff417b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T00:22:16.281250Z",
     "iopub.status.busy": "2024-04-28T00:22:16.280850Z",
     "iopub.status.idle": "2024-04-28T00:22:16.295829Z",
     "shell.execute_reply": "2024-04-28T00:22:16.295074Z",
     "shell.execute_reply.started": "2024-04-28T00:22:16.281208Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b839aa-8e8a-4c6c-95b0-a2d52dd23fa9",
   "metadata": {},
   "source": [
    "# Load data and build an index + Storing your index + Query your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44d5c4b-5492-4605-b4c3-ea3139558312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T00:22:16.299272Z",
     "iopub.status.busy": "2024-04-28T00:22:16.299001Z",
     "iopub.status.idle": "2024-04-28T00:22:17.929199Z",
     "shell.execute_reply": "2024-04-28T00:22:17.928651Z",
     "shell.execute_reply.started": "2024-04-28T00:22:16.299249Z"
    }
   },
   "outputs": [],
   "source": [
    "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage, get_response_synthesizer\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# documents = SimpleDirectoryReader(llamaindex_dir + \"/data\").load_data()\n",
    "# index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca88744e-3a87-40d2-9bec-5fd515bd529e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T00:22:17.932438Z",
     "iopub.status.busy": "2024-04-28T00:22:17.929947Z",
     "iopub.status.idle": "2024-04-28T00:22:18.693112Z",
     "shell.execute_reply": "2024-04-28T00:22:18.692651Z",
     "shell.execute_reply.started": "2024-04-28T00:22:17.932410Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(llamaindex_dir + \"/data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    # saving the embeddings to disk\n",
    "    # By default, this will save the data to the directory storage, but you can change that by passing a persist_dir parameter.\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94652a-8bd2-45a1-ac10-44d1613a5054",
   "metadata": {},
   "source": [
    "```python\n",
    "# Either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75910b5b-9b1a-49fe-9d1c-00269813f476",
   "metadata": {},
   "source": [
    "How to get lot of data when you have relevant results but potentially no data if you have nothing relevant\n",
    "- we customize our retriever to use a different number for top_k\n",
    "  - For a custom retriever, we use `RetrieverQueryEngine`.\n",
    "- and add a post-processing step that requires that the retrieved nodes reach a minimum similarity score to be included\n",
    "  - For the post-processing step, we use `SimilarityPostprocessor`\n",
    " \n",
    "\n",
    "[Response Synthesizer](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/): A Response Synthesizer is what generates a response from an LLM, using a user query and a given set of text chunks. The output of a response synthesizer is a Response object. When used in a query engine, the response synthesizer is used after nodes are retrieved from a retriever, and after any node-postprocessors are ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae90b44-678d-4b6f-9e41-ff386a143dba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T00:22:18.695614Z",
     "iopub.status.busy": "2024-04-28T00:22:18.695292Z",
     "iopub.status.idle": "2024-04-28T00:22:24.439167Z",
     "shell.execute_reply": "2024-04-28T00:22:24.438426Z",
     "shell.execute_reply.started": "2024-04-28T00:22:18.695595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author engaged in writing and programming activities during their formative years.\n"
     ]
    }
   ],
   "source": [
    "# build index\n",
    "# index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd5bf03-eb75-49c7-9343-73f116443844",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T00:22:24.440840Z",
     "iopub.status.busy": "2024-04-28T00:22:24.440433Z",
     "iopub.status.idle": "2024-04-28T00:22:24.474561Z",
     "shell.execute_reply": "2024-04-28T00:22:24.473705Z",
     "shell.execute_reply.started": "2024-04-28T00:22:24.440811Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(RetrieverQueryEngine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a02438-e3a4-4891-9e9c-4047588f1915",
   "metadata": {},
   "source": [
    "# Configuring retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06324cd-138a-497c-8d83-74c3917588c2",
   "metadata": {},
   "source": [
    "There are a huge variety of retrievers that you can learn about in our [module guide on retrievers](https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065eae8c-34ea-42ae-832e-77a17fc9a7f6",
   "metadata": {},
   "source": [
    "# Configuring node postprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7f183-165f-41d1-8a66-f5598d27e684",
   "metadata": {},
   "source": [
    "The full list of node postprocessors is documented in the [Node Postprocessor Reference](https://docs.llamaindex.ai/en/stable/api_reference/postprocessor/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f068a-78b2-4952-9d2a-b1b52332d09a",
   "metadata": {},
   "source": [
    "# Configuring response synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c63d0-5a04-4950-b422-7bceda6a593b",
   "metadata": {},
   "source": [
    "After a retriever fetches relevant nodes, a BaseSynthesizer synthesizes the final response by combining the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da39adc-e2e2-4fd3-8d5a-4d6deb85f171",
   "metadata": {},
   "source": [
    "# Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6220b-3c84-4e53-9442-acadd66c4b66",
   "metadata": {},
   "source": [
    "# Creating your own Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c7f54-4b13-4723-9e01-2e0614bb5b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
