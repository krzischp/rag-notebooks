{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977cad9-5c46-4836-a58e-6e6ee2c370a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac89ca9-08d4-4f9c-a908-a823f5fcf22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eebf7cae-6e1a-4f40-bed1-0c840fc4e532",
   "metadata": {},
   "source": [
    "[Tutorial link](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf095f85-1b9a-4430-86bc-28d80bc8037e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T19:52:26.554668Z",
     "iopub.status.busy": "2024-05-01T19:52:26.551893Z",
     "iopub.status.idle": "2024-05-01T19:52:26.616043Z",
     "shell.execute_reply": "2024-05-01T19:52:26.615204Z",
     "shell.execute_reply.started": "2024-05-01T19:52:26.554481Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d771622-8d4f-4522-83dd-4ef5eca15c82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T19:52:26.715474Z",
     "iopub.status.busy": "2024-05-01T19:52:26.715139Z",
     "iopub.status.idle": "2024-05-01T19:52:27.319189Z",
     "shell.execute_reply": "2024-05-01T19:52:27.318524Z",
     "shell.execute_reply.started": "2024-05-01T19:52:26.715444Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0441b9c-9ce7-429e-83b7-97a96f1dec64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T19:55:39.016643Z",
     "iopub.status.busy": "2024-05-01T19:55:39.016317Z",
     "iopub.status.idle": "2024-05-01T19:55:39.046071Z",
     "shell.execute_reply": "2024-05-01T19:55:39.044693Z",
     "shell.execute_reply.started": "2024-05-01T19:55:39.016617Z"
    }
   },
   "outputs": [],
   "source": [
    "def navigate_up(path, levels):\n",
    "    \"\"\"Navigate up `levels` directories from the given path.\"\"\"\n",
    "    for _ in range(levels):\n",
    "        path = os.path.dirname(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "084e8e01-65bc-4d64-a5fc-b3eefdef02e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T19:55:39.175473Z",
     "iopub.status.busy": "2024-05-01T19:55:39.175196Z",
     "iopub.status.idle": "2024-05-01T19:55:39.207072Z",
     "shell.execute_reply": "2024-05-01T19:55:39.206385Z",
     "shell.execute_reply.started": "2024-05-01T19:55:39.175452Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "llamaindex_dir = os.getcwd()\n",
    "# Get the parent directory\n",
    "llamaindex_dir = os.path.dirname(llamaindex_dir)\n",
    "\n",
    "sys.path.append(llamaindex_dir + \"/utils\")\n",
    "sys.path.append(navigate_up(llamaindex_dir, 2) + \"/law-sec-insights/backend\")\n",
    "# sys.path\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "351708be-dd65-47a1-b281-1e4b2be5265f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T19:55:49.229217Z",
     "iopub.status.busy": "2024-05-01T19:55:49.228928Z",
     "iopub.status.idle": "2024-05-01T19:55:49.258344Z",
     "shell.execute_reply": "2024-05-01T19:55:49.257328Z",
     "shell.execute_reply.started": "2024-05-01T19:55:49.229195Z"
    }
   },
   "outputs": [],
   "source": [
    "from llamaindex_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89691402-64a9-4dab-a536-14e864ff417b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T19:55:49.400468Z",
     "iopub.status.busy": "2024-05-01T19:55:49.400138Z",
     "iopub.status.idle": "2024-05-01T19:55:49.429194Z",
     "shell.execute_reply": "2024-05-01T19:55:49.427911Z",
     "shell.execute_reply.started": "2024-05-01T19:55:49.400441Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b839aa-8e8a-4c6c-95b0-a2d52dd23fa9",
   "metadata": {},
   "source": [
    "# Load data and build an index + Storing your index + Query your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44d5c4b-5492-4605-b4c3-ea3139558312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T19:52:27.479526Z",
     "iopub.status.busy": "2024-05-01T19:52:27.479158Z",
     "iopub.status.idle": "2024-05-01T19:52:29.712058Z",
     "shell.execute_reply": "2024-05-01T19:52:29.711586Z",
     "shell.execute_reply.started": "2024-05-01T19:52:27.479497Z"
    }
   },
   "outputs": [],
   "source": [
    "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage, get_response_synthesizer\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# documents = SimpleDirectoryReader(llamaindex_dir + \"/data\").load_data()\n",
    "# index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca88744e-3a87-40d2-9bec-5fd515bd529e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T19:52:29.714086Z",
     "iopub.status.busy": "2024-05-01T19:52:29.713426Z",
     "iopub.status.idle": "2024-05-01T19:52:30.575122Z",
     "shell.execute_reply": "2024-05-01T19:52:30.574590Z",
     "shell.execute_reply.started": "2024-05-01T19:52:29.714054Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(llamaindex_dir + \"/data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    # saving the embeddings to disk\n",
    "    # By default, this will save the data to the directory storage, but you can change that by passing a persist_dir parameter.\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94652a-8bd2-45a1-ac10-44d1613a5054",
   "metadata": {},
   "source": [
    "```python\n",
    "# Either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75910b5b-9b1a-49fe-9d1c-00269813f476",
   "metadata": {},
   "source": [
    "How to get lot of data when you have relevant results but potentially no data if you have nothing relevant\n",
    "- we customize our retriever to use a different number for top_k\n",
    "  - For a custom retriever, we use `RetrieverQueryEngine`.\n",
    "- and add a post-processing step that requires that the retrieved nodes reach a minimum similarity score to be included\n",
    "  - For the post-processing step, we use `SimilarityPostprocessor`\n",
    " \n",
    "\n",
    "[Response Synthesizer](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/): A Response Synthesizer is what generates a response from an LLM, using a user query and a given set of text chunks. The output of a response synthesizer is a Response object. When used in a query engine, the response synthesizer is used after nodes are retrieved from a retriever, and after any node-postprocessors are ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae90b44-678d-4b6f-9e41-ff386a143dba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T19:52:30.575985Z",
     "iopub.status.busy": "2024-05-01T19:52:30.575826Z",
     "iopub.status.idle": "2024-05-01T19:52:35.649183Z",
     "shell.execute_reply": "2024-05-01T19:52:35.648392Z",
     "shell.execute_reply.started": "2024-05-01T19:52:30.575968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing and programming before college.\n"
     ]
    }
   ],
   "source": [
    "# build index\n",
    "# index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd5bf03-eb75-49c7-9343-73f116443844",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T00:22:24.440840Z",
     "iopub.status.busy": "2024-04-28T00:22:24.440433Z",
     "iopub.status.idle": "2024-04-28T00:22:24.474561Z",
     "shell.execute_reply": "2024-04-28T00:22:24.473705Z",
     "shell.execute_reply.started": "2024-04-28T00:22:24.440811Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(RetrieverQueryEngine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289eb2b8-cabb-4532-91f5-857c4741496d",
   "metadata": {},
   "source": [
    "# Try in agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81be3965-85f7-4cfc-908e-3280ea848358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T20:33:43.196339Z",
     "iopub.status.busy": "2024-05-01T20:33:43.195978Z",
     "iopub.status.idle": "2024-05-01T20:33:43.225100Z",
     "shell.execute_reply": "2024-05-01T20:33:43.224319Z",
     "shell.execute_reply.started": "2024-05-01T20:33:43.196310Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"lyft_10k\",\n",
    "            description=(\n",
    "                # NOT WORKING\n",
    "                # \"Provides information about Lyft financials for year 2021. \"\n",
    "                # \"Use a detailed plain text question as input to the tool.\"\n",
    "                # WORKING!!!\n",
    "                \"Paul Graham essay on What I Worked On\"\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79e0a124-9b25-4139-8df0-408d76ca2b0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T20:33:43.857655Z",
     "iopub.status.busy": "2024-05-01T20:33:43.857332Z",
     "iopub.status.idle": "2024-05-01T20:33:43.886013Z",
     "shell.execute_reply": "2024-05-01T20:33:43.885152Z",
     "shell.execute_reply.started": "2024-05-01T20:33:43.857631Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c3d9ae4-dc79-44c0-9964-163f1113ab58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T20:33:44.176774Z",
     "iopub.status.busy": "2024-05-01T20:33:44.176460Z",
     "iopub.status.idle": "2024-05-01T20:33:44.224939Z",
     "shell.execute_reply": "2024-05-01T20:33:44.224285Z",
     "shell.execute_reply.started": "2024-05-01T20:33:44.176752Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = OpenAIAgent.from_tools(query_engine_tools, verbose=True)\n",
    "# PB: THE AGENT CAN FAIL WHEN CHOOSING THE TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "032230a6-ada0-48b5-9993-a15da938bb46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T20:35:29.185524Z",
     "iopub.status.busy": "2024-05-01T20:35:29.184982Z",
     "iopub.status.idle": "2024-05-01T20:35:42.288385Z",
     "shell.execute_reply": "2024-05-01T20:35:42.287331Z",
     "shell.execute_reply.started": "2024-05-01T20:35:29.185501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: lyft_10k with args: {\n",
      "  \"input\": \"Paul Graham\"\n",
      "}\n",
      "Got output: Paul Graham is recognized for his contributions to software development and entrepreneurship, including projects like Viaweb and the development of Bel, a Lisp dialect. Additionally, Graham is known for his thought-provoking essays covering a wide range of topics in the tech industry and startup ecosystem.\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agent.chat_repl()\n",
    "response = agent.chat(\"What did Paul Grahams do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7430d480-fc98-4a42-a0a0-2dedcddc0521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T20:35:43.050177Z",
     "iopub.status.busy": "2024-05-01T20:35:43.049843Z",
     "iopub.status.idle": "2024-05-01T20:35:43.078141Z",
     "shell.execute_reply": "2024-05-01T20:35:43.077511Z",
     "shell.execute_reply.started": "2024-05-01T20:35:43.050150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growing up, Paul Graham had a diverse range of interests and experiences. He developed a passion for programming at a young age and became proficient in languages like BASIC, Pascal, and Lisp. He also excelled in mathematics and participated in math competitions. Graham was an avid reader and writer, which led him to pursue a degree in English at Cornell University. He also engaged in entrepreneurial ventures, starting small businesses and running a summer camp. During his academic years at Harvard University, he studied philosophy and computer science, conducting research and innovation projects. Graham's notable contributions include projects like Viaweb, the first web-based application, and the development of Bel, a Lisp dialect. He is also known for his thought-provoking essays on various topics in the tech industry and entrepreneurship.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cdaad8b6-521a-45d8-9073-ee1de2ed0f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T20:36:20.260245Z",
     "iopub.status.busy": "2024-05-01T20:36:20.259954Z",
     "iopub.status.idle": "2024-05-01T20:36:20.287208Z",
     "shell.execute_reply": "2024-05-01T20:36:20.286520Z",
     "shell.execute_reply.started": "2024-05-01T20:36:20.260222Z"
    }
   },
   "outputs": [],
   "source": [
    "# response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a02438-e3a4-4891-9e9c-4047588f1915",
   "metadata": {},
   "source": [
    "# Configuring retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06324cd-138a-497c-8d83-74c3917588c2",
   "metadata": {},
   "source": [
    "There are a huge variety of retrievers that you can learn about in our [module guide on retrievers](https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065eae8c-34ea-42ae-832e-77a17fc9a7f6",
   "metadata": {},
   "source": [
    "# Configuring node postprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7f183-165f-41d1-8a66-f5598d27e684",
   "metadata": {},
   "source": [
    "The full list of node postprocessors is documented in the [Node Postprocessor Reference](https://docs.llamaindex.ai/en/stable/api_reference/postprocessor/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f068a-78b2-4952-9d2a-b1b52332d09a",
   "metadata": {},
   "source": [
    "# Configuring response synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c63d0-5a04-4950-b422-7bceda6a593b",
   "metadata": {},
   "source": [
    "After a retriever fetches relevant nodes, a BaseSynthesizer synthesizes the final response by combining the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da39adc-e2e2-4fd3-8d5a-4d6deb85f171",
   "metadata": {},
   "source": [
    "# Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6220b-3c84-4e53-9442-acadd66c4b66",
   "metadata": {},
   "source": [
    "# Creating your own Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c7f54-4b13-4723-9e01-2e0614bb5b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
